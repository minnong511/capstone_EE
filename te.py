import torch
import torchaudio
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

import os

file_path = "flitered_signal.wav"
print("íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:", os.path.exists(file_path))  # Trueì—¬ì•¼ í•¨

# ëª¨ë¸ ë¡œë”©
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# ì˜¤ë””ì˜¤ ë¶ˆëŸ¬ì˜¤ê¸°
waveform, sample_rate = torchaudio.load("flitered_signal.wav")

# ë¦¬ìƒ˜í”Œë§ (16kHz í•„ìˆ˜)
if sample_rate != 16000:
    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
    waveform = resampler(waveform)

# ëª¨ë…¸ ì²˜ë¦¬: [1, time]
if waveform.shape[0] > 1:
    waveform = torch.mean(waveform, dim=0, keepdim=True)

# ë°°ì¹˜ ì°¨ì› ì—†ì´ ë²¡í„°ë¡œ ë³€í™˜: [time]
waveform = waveform.squeeze()

# ì…ë ¥ ì „ì²˜ë¦¬
inputs = processor(waveform, sampling_rate=16000, return_tensors="pt")

# ì¶”ë¡ 
with torch.no_grad():
    logits = model(**inputs).logits

# ë””ì½”ë”©
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.decode(predicted_ids[0])

print("ğŸ—£ï¸ ì¸ì‹ëœ ë¬¸ì¥:", transcription)
